\pdfoutput=1

\documentclass[11pt]{article}

\usepackage[]{ACL2023}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\title{Asking Clarifying Questions for Conversational Search}

\author{
  Ansh Bisht \and
  Jordan Dickson \and
  Jack Harrison \and
  Ruben Lazell \and
  Andrew Taison  \\
  School of Computing, Engineering and the Built Environment, \\
  Edinburgh Napier University \\
  Matric Numbers: 40527530, 40545300, 40537035, 40679914, 40538519
}


\begin{document}
\maketitle
\begin{abstract}
Short abstract.
\end{abstract}


\section{Introduction}
Brief context of problem - why clarifying Q's matter.
What is the goal.
Structure of the paper.

\section{Related Work}
This project is motivated by recent work in conversational search, particularly where ambiguity in user queries is addressed by asking clarifying questions. Across the discussed literature it is commonly noted that users often struggle to express their intent clearly in a single query and find that even simple follow up questions can help search systems return more relevant results. Whilst our system simplifies many of the models used in previous work, it is inspired by several of their core ideas, adapting them to be lightweight for demonstration purposes and time constraints.

\subsection{Clarification as a Ranking Problem and the Qulac Dataset}
\citet{Aliannejadi2019} examine how clarifying questions can improve conversational search, showing that even a single, well targeted question can significantly improve retrieval performance. To support this, they introduce the Qulac dataset, which extends TREC Web Track topics with crowd sourced question and answer pairs organised by query facets (categories). This dataset has been widely used in related research. In addition to the dataset, they propose a retrieval framework in which both documents and candidate questions are retrieved and ranked based on the user's query and conversational context. Question ranking is performed using a BERT model pre-trained on Wikipedia and fine-tuned on Qulac.

Our system builds on this framework, making use of the Qulac dataset for its high quality question-answer pairs and \citeauthor{Aliannejadi2019}'s multi-turn conversation extension dataset. Like their model, we treat clarification as a ranking problem, selecting the most relevant question from a fixed set based on the current context, rather than generating questions from scratch.

\subsection{Simulated Interaction and Clarification Behaviour}
Alongside ranking-based clarification frameworks, other studies have focused on how user behaviour can affect the system and effectiveness. Two notable examples are the CoSearcher system introduced by \citet{Salle2022} and the risk-aware decision model proposed by \citet{Wang2022}. Whilst these works differ in their objectives, both incorporate user simulators and address when and how clarification should take place in conversational search.

\citet{Salle2022} present CoSearcher, a user simulator designed to evaluate search intent refinement in conversational search by modelling different user behaviours. Their main focus is on testing how different facet-ranking strategies perform under varying conditions such as low user patience or reduced cooperativeness. The authors experiment with a range of ranking models, including LexVec, and use BERT to classify user responses and determine whether further clarification is necessary. Although SBERT was initially considered, it was not used, as LexVec was found to slightly outperform it.

\citet{Wang2022} approach the clarification process as a decision making problem, where the system has to decide whether to ask a question or return a result based on how confident it is. To explore this, they train a reinforcement learning model using a reward function that balances retrieval performance with the length and usefulness of the conversation. Like CoSearcher, they implement a user simulator to represent different behaviours, including tolerance and patience. They use these two parameters to determine how many questions a user will put up with before giving up.

Both studies highlight the importance of choosing not just what to ask, but when to ask it. Whilst our system does not include a user simulator or learning-based decision model, we were influenced by how these papers handle ambiguity and user behaviour. In our case, the decision to clarify or move on is made using a straightforward confidence check from the BM25 module. If the score is high, we assume the summary is a good enough match for the user's intent. If it is low, the system switches to question selection instead. Whilst this is a much simpler method, it follows the same general idea that clarification should depend on context and confidence, rather than being the default.






\section{Methodology}
System architecture.
What models/algorithms used.
How are the questions generated.
What inputs/outputs are there.

\cite{Aliannejadi2019}

\section{Evaluation}
How did we check the system works.
Describe testing process.

\section{Results and Discussion}
Sample outputs.
Summary of system behaviour.
What worked well, what didn't?
Possible improvements.

\section{Conclusion}
start here.

\section*{Limitations}
Required by ACL format, and should be AFTER conclusion.
Discuss honest limitations of the work.

\section*{Ethics Statement}
Required by ACL format. Could just be a sentence or two.
Explicit ethics statement on the broader impact of the work, or other ethical considerations.

\bibliographystyle{acl_natbib}
\bibliography{references}

\appendix

\section{Appendix}
Possibly not needed.

\end{document}